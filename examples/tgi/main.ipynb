{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark sglang Server with inference-perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local TGI Setup using docker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run TGI Server as a docker container with the model HuggingFace `HuggingFaceTB/SmolLM2-135M-Instruct`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "!docker-compose up -d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark with inference_perf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a configuration file for the test using `shareGPT` data and run the constant rate test for `30s`. You can also use any of the other data generators like `random`, `shared-prefix` or `synthetic` with their own configuration using the corresponding `config-*.yml` file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:\n",
      "  type: shareGPT\n",
      "load:\n",
      "  type: constant\n",
      "  stages:\n",
      "  - rate: 1\n",
      "    duration: 30\n",
      "api: \n",
      "  type: chat\n",
      "server:\n",
      "  type: tgi\n",
      "  model_name: HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "  base_url: http://0.0.0.0:8000\n",
      "metrics:\n",
      "  type: prometheus\n",
      "  prometheus:\n",
      "    url: http://localhost:9090\n",
      "    scrape_interval: 15"
     ]
    }
   ],
   "source": [
    "!cat config.yml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n",
      "2025-09-01 22:47:30,024 - inference_perf.config - INFO - Using configuration from: config.yml\n",
      "2025-09-01 22:47:30,027 - inference_perf.config - INFO - Benchmarking with the following config:\n",
      "\n",
      "api:\n",
      "  type: chat\n",
      "  streaming: false\n",
      "  headers: null\n",
      "data:\n",
      "  type: shareGPT\n",
      "  path: null\n",
      "  input_distribution: null\n",
      "  output_distribution: null\n",
      "  shared_prefix: null\n",
      "load:\n",
      "  type: constant\n",
      "  interval: 1.0\n",
      "  stages:\n",
      "  - rate: 1\n",
      "    duration: 30\n",
      "  num_workers: 32\n",
      "  worker_max_concurrency: 100\n",
      "  worker_max_tcp_connections: 2500\n",
      "metrics:\n",
      "  type: prometheus\n",
      "  prometheus:\n",
      "    url: http://localhost:9090\n",
      "    scrape_interval: 15\n",
      "report:\n",
      "  request_lifecycle:\n",
      "    summary: true\n",
      "    per_stage: true\n",
      "    per_request: false\n",
      "  prometheus:\n",
      "    summary: true\n",
      "    per_stage: false\n",
      "storage:\n",
      "  local_storage:\n",
      "    path: reports-20250901-224725\n",
      "    report_file_prefix: null\n",
      "  google_cloud_storage: null\n",
      "  simple_storage_service: null\n",
      "server:\n",
      "  type: tgi\n",
      "  model_name: HuggingFaceTB/SmolLM2-135M-Instruct\n",
      "  base_url: http://0.0.0.0:8000\n",
      "tokenizer: null\n",
      "\n",
      "\n",
      "2025-09-01 22:47:30,058 - inference_perf.client.filestorage.local - INFO - Report files will be stored at: reports-20250901-224725\n",
      "2025-09-01 22:47:50,653 - inference_perf.loadgen.load_generator - INFO - Stage 0 - run started\n",
      "2025-09-01 22:48:23,079 - inference_perf.loadgen.load_generator - INFO - Stage 0 - run completed\n",
      "2025-09-01 22:48:23,088 - inference_perf.reportgen.base - INFO - Generating Reports...\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: avg_time_to_first_token. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: median_time_to_first_token. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: p90_time_to_first_token. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: p99_time_to_first_token. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: avg_kv_cache_usage. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: median_kv_cache_usage. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: p90_kv_cache_usage. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: p99_kv_cache_usage. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: avg_inter_token_latency. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: median_inter_token_latency. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: p90_inter_token_latency. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: p99_inter_token_latency. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: num_preemptions_total. Skipping this metric.\n",
      "2025-09-01 22:48:40,134 - inference_perf.client.metricsclient.prometheus_client.base - WARNING - Metric metadata is not present for metric: num_requests_swapped. Skipping this metric.\n",
      "2025-09-01 22:48:40,135 - inference_perf.client.filestorage.local - INFO - Report saved to: reports-20250901-224725/summary_lifecycle_metrics.json\n",
      "2025-09-01 22:48:40,135 - inference_perf.client.filestorage.local - INFO - Report saved to: reports-20250901-224725/stage_0_lifecycle_metrics.json\n",
      "2025-09-01 22:48:40,135 - inference_perf.client.filestorage.local - INFO - Report saved to: reports-20250901-224725/summary_prometheus_metrics.json\n"
     ]
    }
   ],
   "source": [
    "!inference-perf --config_file config.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View benchmark metrics in the reports folder created."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete vLLM Server docker processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!docker-compose down"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "inference-perf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
